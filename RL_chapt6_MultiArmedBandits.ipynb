{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiarmed bandits \n",
    "\n",
    "# the casino machine has levers.\n",
    "# each machine has a lever has an unknown probability distribution\n",
    "# and can yield a reward potentially from it's probability distirbution. \n",
    "# ONE slot machine = one armed bandit\n",
    "# Many slot machines = multi / k - armed bandits\n",
    "\n",
    "# Goal: find out which slot machine gives us maximum cumulative reward over time.\n",
    "\n",
    "\n",
    "# So at each time step, agent does an action (pulls slot machine arm + recieve reward) \n",
    "# the value of an action is: \n",
    "# Q_value(action) = sum of rewards from machine / total number of arms pulled on machine. \n",
    "\n",
    "# So the best slot machine is:\n",
    "# Max(Q_value(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_bandits\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-10-25 11:56:21,292] Making new env: BanditTenArmedGaussian-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BanditTenArmedGaussian-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space # we have 10 arms to choose from, so action space is 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration algorithms are used to solve the M-A-B problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Epsilon Greedy exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds = 20000\n",
    "\n",
    "# metrics to keep a track of \n",
    "count = np.zeros(10)\n",
    "sum_rewards = np.zeros(10)\n",
    "Q = np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(epsilon):\n",
    "    rand = np.random.random()\n",
    "    if rand < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(Q)\n",
    "    \n",
    "    return(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal arm is 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_rounds):\n",
    "    # select arm using epsilon greedy\n",
    "    arm = epsilon_greedy(0.5)\n",
    "    # get reward\n",
    "    observation, reward, done, info = env.step(arm)\n",
    "    # update the count of that arm\n",
    "    count[arm] += 1 \n",
    "    # sum the rewards obtained from the arm\n",
    "    sum_rewards[arm] += reward\n",
    "    # calculate Q value which is the average rewards of the arm\n",
    "    Q[arm] = sum_rewards[arm] / count[arm]\n",
    "\n",
    "print(\"The optimal arm is {}\".format(np.argmax(Q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Softmax exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax exploration. / \"Boltzman exploration\"\n",
    "\n",
    "# we select the best arm based on probability from botlzman distribution\n",
    "\n",
    "# the probability of selecting an arm is given by equation:\n",
    "\n",
    "# tau # is a parameter that is set\n",
    "# t # temperature factor wich specify how many random arms can be explored. \n",
    "#   - when t is high all arms are explored equally\n",
    "#   - when t is low, only arms with high rewards are chosen.\n",
    "\n",
    "# Pt(arm) = exp(Qt(arm)/tau)  /  sum[exp(Qt(i)/tau)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as math\n",
    "import random as random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(tau):\n",
    "    total = sum([math.exp(val/tau) for val in Q]) # denominator\n",
    "    probs = [math.exp(val/tau)/total for val in Q] # numerator  # val is Q value - i.e Qt(arm) \n",
    "    \n",
    "    # probs = every Qt(arm) value has a probability associated with it. \n",
    "    \n",
    "    threshold = random.random()\n",
    "    cumulative_prob = 0.0 \n",
    "    for i in range(len(probs)):           # for all probabilities for all actions...  \n",
    "        cumulative_prob += probs[i]        # collect cumulative probability \n",
    "        if (cumulative_prob > threshold):  # once the cumulative probability reaches a threshold - return i.  \n",
    "            return i\n",
    "    return np.argmax(probs)   # else return the highest porbability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal arm is 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_rounds):\n",
    "    # select arm using epsilon greedy\n",
    "    arm = softmax(0.5)\n",
    "    # get reward\n",
    "    observation, reward, done, info = env.step(arm)\n",
    "    # update the count of that arm\n",
    "    count[arm] += 1 \n",
    "    # sum the rewards obtained from the arm\n",
    "    sum_rewards[arm] += reward\n",
    "    # calculate Q value which is the average rewards of the arm\n",
    "    Q[arm] = sum_rewards[arm] / count[arm]\n",
    "\n",
    "print(\"The optimal arm is {}\".format(np.argmax(Q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper confidence bound algorithm\n",
    "\n",
    "- sometimes actions which do not initially give good rewards may actually pay off in the long term. \n",
    "- this is based on the principle of optimism in the face of uncertainty. \n",
    "- we need to pull an arm multiple times to find out the true distribution of rewards the arm gives - despite priors\n",
    "- the confidence interval species the range in which the mean reward of the value of the arm lies \n",
    "- between the LOWER confidence bound and the UPPER confidence bound is where the mean lies...\n",
    "\n",
    "\n",
    "- UCB algorithm chooses arm / action that has highest sum of average reward and upper confidence bound. \n",
    "- whilst also updating arms reward and confidence bound after each action-reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formula for upper condidence bound:\n",
    "#  UCB = square root ( 2log(t)/ N(a))\n",
    "# where N is number of times the action is performed / arm puled. \n",
    "\n",
    "\n",
    "# An action / arm is chosen with\n",
    "# Arm = argmax [Q(a) + UCB]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCB(iters, num_possible_actions=10):\n",
    "    '''\n",
    "    There are 10 slot machines so num_possible_actions = 10 by default\n",
    "    '''\n",
    "    ucb = np.zeros(num_possible_actions)\n",
    "    # explore all the arms\n",
    "    if iters < num_possible_actions:\n",
    "        return i\n",
    "    else: \n",
    "        for arm in range(num_possible_actions):\n",
    "            # calculate upper bound\n",
    "            upper_bound = math.sqrt((2*math.log(sum(count))) / count[arm])\n",
    "            # add upper bound to the Q value\n",
    "            ucb[arm] = Q[arm] + upper_bound\n",
    "        # retyrn the arm which has maximum value\n",
    "        return(np.argmax(ucb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal arm is 1\n"
     ]
    }
   ],
   "source": [
    "num_rounds = 20000\n",
    "count = np.zeros(10) # number of times arm is pulled - used within scope of UCB function. \n",
    "sum_rewards = np.zeros(10)\n",
    "Q = np.zeros(10)\n",
    "\n",
    "for i in range(num_rounds):\n",
    "    arm = UCB(i, num_possible_actions=10)\n",
    "    # get reward\n",
    "    observation, reward, done, info = env.step(arm)\n",
    "    # update count of arm\n",
    "    count[arm] += 1\n",
    "    # sum rewards obtained from the arm\n",
    "    sum_rewards[arm] += reward\n",
    "    # calculate q value which is the average reward of the arm\n",
    "    Q[arm] = sum_rewards[arm] / count[arm]\n",
    "\n",
    "print(\"The optimal arm is {}\".format(np.argmax(Q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thompson sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds = 20000\n",
    "count = np.zeros(10) # number of times arm is pulled - used within scope of UCB function. \n",
    "sum_rewards = np.zeros(10)\n",
    "Q = np.zeros(10)\n",
    "\n",
    "# initialise alpha and beta values:\n",
    "alpha = np.ones(10)\n",
    "beta = np.ones(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thompson_sampling(alpha, beta, num_possible_actions=10):\n",
    "    samples = [np.random.beta(alpha[i] + 1, beta[i] + 1) for i in range(num_possible_actions)]\n",
    "    return np.argmax(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal arm is 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_rounds):\n",
    "    arm = thompson_sampling(alpha, beta, num_possible_actions=10)\n",
    "    # get reward\n",
    "    observation, reward, done, info = env.step(arm)\n",
    "    # update count of arm\n",
    "    count[arm] += 1\n",
    "    # sum rewards obtained from the arm\n",
    "    sum_rewards[arm] += reward\n",
    "    # calculate q value which is the average reward of the arm\n",
    "    Q[arm] = sum_rewards[arm] / count[arm]\n",
    "    \n",
    "    if reward > 0 : \n",
    "        alpha[arm] += 1\n",
    "    else:\n",
    "        beta[arm] += 1\n",
    "\n",
    "print(\"The optimal arm is {}\".format(np.argmax(Q)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:will_tun]",
   "language": "python",
   "name": "conda-env-will_tun-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
